{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 7 - Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy \n",
    "import config\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import textblob\n",
    "import time\n",
    "\n",
    "# Import and Initialize Sentiment Analyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Twitter API Keys\n",
    "consumer_key = config.consAPIkey\n",
    "consumer_secret = config.consAPISecret\n",
    "access_token = config.accessToken\n",
    "access_token_secret = config.accessTokenSecret\n",
    "\n",
    "# Setup Tweepy API Authentication\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth, parser=tweepy.parsers.JSONParser())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textAnalyzer(target_string):\n",
    "    compound = analyzer.polarity_scores(target_string)[\"compound\"]\n",
    "    pos = analyzer.polarity_scores(target_string)[\"pos\"]\n",
    "    neu = analyzer.polarity_scores(target_string)[\"neu\"]\n",
    "    neg = analyzer.polarity_scores(target_string)[\"neg\"]\n",
    "\n",
    "    # Print Analysis\n",
    "    #print(\"\\n\\n\"+target_string+\"\\n\\n\")\n",
    "    #print(\"Compound Score: %s\" % compound)\n",
    "    #print(\"Positive Score: %s\" % pos)\n",
    "    #print(\"Neutral Score: %s\" % neu)\n",
    "    #print(\"Negative Score: %s\" % neg)\n",
    "    return {\"text\":target_string,\"compound\":compound,\"pos\":pos,\"neu\":neu,\"neg\":neg}\n",
    "\n",
    "def makePlot(x,y,figname,target):\n",
    "    ypol = []\n",
    "    for tweet in y:\n",
    "        #blob = textblob.TextBlob(tweet)\n",
    "        VADERAnalysis = textAnalyzer(tweet)\n",
    "        compound = VADERAnalysis['compound']\n",
    "        ypol.append(compound)\n",
    "    plt.plot(x,ypol,'-o')\n",
    "    plt.xlabel(\"Tweet #\")\n",
    "    plt.ylabel(\"Compound Sentiment\")\n",
    "    plt.title(target+\" sentiment (compound)\")\n",
    "    plt.savefig(figname)\n",
    "    plt.close()\n",
    "    \n",
    "def resondToTweet(filename,status_id,target,user):\n",
    "    statusText = \"@\"+user+ \" Hello! Here is your requested analysis of \"+target\n",
    "    api.update_with_media(filename=filename,in_reply_to_status_id_str=status_id,status=statusText)\n",
    "    \n",
    "def getOnlyNewTweets(history,newMentions):\n",
    "    return_list_new = []\n",
    "    for i in range(len(newMentions)):\n",
    "        if newMentions[i]['id'] in [item['id'] for item in history]:\n",
    "            pass\n",
    "        elif newMentions[i]['text'].split(\":\")[-1].strip() in [item['text'].split(\":\")[-1].strip() for item in history]:\n",
    "            pass\n",
    "        else:\n",
    "            return_list_new.append(newMentions[i])\n",
    "    return return_list_new\n",
    "\n",
    "def analyzeTweetsAndSend(tweet_dict):\n",
    "    user = tweet_dict['user']['screen_name'] #User that requested analysis\n",
    "    id_str = tweet_dict['id_str'] #This is the id of the tweet that requested analysis\n",
    "    filename = id_str+\".png\" #This will be the file name that we will use to save the figure of the analysis\n",
    "    analysis_target = tweet_dict['text'].split(\":\")[-1].strip() #This is the target who's tweets we will analyze\n",
    "    tweet_text = []\n",
    "    for x in range(25):\n",
    "        # Get all tweets from target user\n",
    "        public_tweets = api.user_timeline(analysis_target, page=x)    \n",
    "        # Loop through all tweets and append tweet text to list\n",
    "        for item in public_tweets:\n",
    "            tweet_text.append(item['text'])\n",
    "    tweet_list_enum = [(i-len(tweet_text)+1,text) for i,text in enumerate(tweet_text)]\n",
    "    x = [item[0] for item in tweet_list_enum]\n",
    "    y = [item[1] for item in tweet_list_enum]\n",
    "    makePlot(x,y,filename,analysis_target)\n",
    "    resondToTweet(filename,id_str,analysis_target,user)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have 3 new mentions!\n",
      "we have 0 new mentions!\n",
      "we have 0 new mentions!\n",
      "we have 0 new mentions!\n",
      "we have 0 new mentions!\n",
      "we have 0 new mentions!\n",
      "we have 1 new mentions!\n",
      "we have 0 new mentions!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-b5b23ba5c24e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0;31m#2. Add new tweet to history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mtweet_history_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_tweets_to_analyze\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#1. Scan for metions every 5 minutes seconds\n",
    "tweet_history_list = []\n",
    "while(True):\n",
    "    # Do a check on the mentions timeline and see if the contain the keyword \n",
    "    # \"Sentiment\" in the text\n",
    "    mentions_list = [item for item in api.mentions_timeline() if \"sentiment\" in item['text'].lower()]\n",
    "    if(len(mentions_list)>0):\n",
    "        #1. Check and return only the tweets that are new AND were from accounts NOT previously analyzed. \n",
    "        new_tweets_to_analyze = getOnlyNewTweets(tweet_history_list,mentions_list)\n",
    "        print(\"we have %d new mentions!\" % (len(new_tweets_to_analyze)))\n",
    "        for i in range(len(new_tweets_to_analyze)):\n",
    "            analyzeTweetsAndSend(new_tweets_to_analyze[i])\n",
    "            #2. Add new tweet to history\n",
    "            tweet_history_list.append(new_tweets_to_analyze[i])\n",
    "    time.sleep(5*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
